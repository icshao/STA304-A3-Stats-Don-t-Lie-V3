---
title: "Title of Your Report"
author: "David Wong, Iris Shao, Jingjing Zhan, Muhammad Tsany"
date: "November 2, 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(here)
library(skimr)
library(kableExtra)

# Loading in the cleaned survey Data
survey_data <- read_csv("C:/Users/irfan/Documents/University/Year 3/Fall 2020/STA304/Problem Set/PS3/outputs/survey_data.csv")

# Loading in the cleaned census Data
census_data <- read_csv("C:/Users/irfan/Documents/University/Year 3/Fall 2020/STA304/Problem Set/PS3/outputs/census_data.csv")

```

# Title of your Report

## Name(s) of Author(s) 
## Date

# Model

## Model Specifics
We based our weights off of the surveyed data from the survey results collected from Nationscape. From modelling the survey data, we will then load in the post-stratification data which is microdata collected by Integrated Public Use Microdata Series (IPUMS). We will be using our model to apply to the post-stratified data. We chose to model our survey data with logistic regression as our model since our outcome of interest, candidate vote, our dependent variable (Y), is assumed to be binary (Donald Trump = 1 and Joe Biden = 0). We can make this assumption despite the presence of other possible candidates because the majority of votes for a candidate in a given state gets all the electoral delegates, regardless of other popular votes. Thus, since the democratic and republican parties hold strong majorities, we are assuming that other parties do not have enough representation to out-vote the other major parties.

Our model is the following:

$$log(\frac{p}{1-p})= \beta_{0} + \beta_{ethnicity}x_{ethnicity \ i} + \beta_{gender}x_{gender\ i}  + \beta_{education}x_{education \ i}+ \beta_{employment}x_{employment\ i}$$

$\text{where } i = 1,2,...n\ and\ p\ =\text{ probability of a Trump vote}$

```{r, include=FALSE}
# Creating the Model

trump_model <- glm(vote_trump ~ as.factor(race_ethnicity_code) + as.factor(education_code) + as.factor(employment_code) + age_group, data = survey_data, family = binomial())

summary(trump_model)


# Model Results (to Report in Results section)
# summary(model)
# OR
# broom::tidy(model)

```

## Post-Stratification 

Multilevel regression with post-stratification (MRP) is a common method used to adjust dummy variables to analyze metrics related to opinion and other survey responses. It leverages and mathematically weighs individual level survey responses (microdata) to various characteristics which is then used to adjust the sample to better represent the population. For our analysis, we followed these steps to post-stratify our data to find our $\hat{y}^{PS}$ values:

1. Grab survey data

2. Clean survey data (quantify predictor variables)

3. Fit a model (glm() with family = binomial() in our case)

4. Grab census data

5. Clean census data  

5.1 Group data by our customized ‘cells’. count the number of individuals in each cell 

5.2 Quantify SAME predictor variables in the SAME way as in 2. For example, if race is divided into 5 categories in survey data, race in census data should also be divided into the SAME 5 categories and the two new columns ‘race_code’ should have SAME name.i.e either both are ‘race_code’ or both are ‘race_ethnicity_code’. )

6. Fit our census data into our model fitted in 3, get $log(\frac{p}{1-p})$ (logodds estimate) where P is the probability of voting for Trump

7. Calculate P based on the $log(\frac{p}{1-p})$ (logodds estimate) calculated in 6

8. Calculate $\hat{Y}$ using the MRP function given in slides

We defined the following as our post-stratification cells: race, age, education, and employment status 

We split race into 5 categories that were generalized by region of ethnicity, if they were more than one, it would be placed into a mixed category. These categories were divided as such: White (1), Black/African American (2), Native American (3), Asian (including Pacific Islanders) (4), mixed races (5).

For education, we categorized the cell into 4 categories based off some assumptions; 1 as 8th grade or less completed, 2 as 9th to 12th grade (high school) completed, 3 as 1 to 4 years of college/university completed (degree or not), and 4 as 5+ years in college completed (degree or not). We are assuming that those completing 4 years most likely will have their degrees and 5+ years would mostly represent those pursuing higher education (MAs, PhDs etc…).

For employment status, we categorized the cell into 3 categories; 1 as employed, 2 as unemployed, 3 as a combination of n/a and not in the labor force. We made this distinction between unemployed (2) and the combo split (3) because the combo split is defined to be people not actively looking. Circumstances for uncertain or non-labor force participants may be housewives, retirement, disabled, in prison etc…; these people aren’t unemployed but rather are classified as n/a or not a part of the labor force via economic conventions.


Goal: $\hat{y}^{PS} = \cfrac{\sum{N_j\hat{y}_j}}{\sum{N_j}}$, where $\hat{y}_j$ is the estimate in each cell and $N_j$ is the population size of the $j^{th}$ cell based off demographics.

## Additional Information

Our analysis is performed in the statistical language R (R Core Team, 2020), using haven (Wickham and Miller, 2019), broom (Robinson and Hayes, 2020), and tidyverse (Wickham et al., 2019) packages. Code and data supporting this analysis is available at: https://github.com/waviddong/STA304-A3-Stats-Don-t-Lie-V3. 

Our model from the survey data reclassified and grouped some categories of our independent variables in order to achieve a statistically significant p-value. Some sub-categories of the given independent variables had very small sample sizes and thus, had a high p-values meaning no statistically significant correlation; when grouped with other similar variables, we were able to get statistically significant p-values. These same techniques for classification would be implemented in the Census Data accordingly.

Our number of Fisher Scoring iterations was 4, meaning that it took 4 iterations until our data fit the model. Thus, it took 4 steps to reach convergence.

```{r, include=TRUE}
# Post-strat calculations

# divide the census data into cells; count the number of individuals in each cell
census_data_group <- 
  census_data %>%
  count(race_ethnicity_code, education_code, employment_code, age_group) %>%
  group_by(race_ethnicity_code, education_code, employment_code, age_group)

# Apply the model fitted above to our poststratification data: we get the log(P/(1-P)) of each cell

# P is the probability of voting for Trump in 2020 election
census_data_group$logodds_estimate <-
  trump_model %>%
  predict(newdata = census_data_group)

# since we used logistic model, we have to convert log(P/(1-P)) to P
census_data_group$estimate <-
  exp(census_data_group$logodds_estimate)/(1+exp(census_data_group$logodds_estimate))

# creating the prediction dataframe for each possible combination of groups
census_data_predict <- census_data_group %>%
  mutate(predict_prop = estimate * n) %>%
  summarise(our_predict = sum(predict_prop)/sum(n))

# calculating final y-hat for the dataset
y_hat<- sum(census_data_predict$our_predict)/sum(nrow(census_data_predict))

```


```{r warning=FALSE}
# tables, graphs, and any other informative plots

# summed mean, 2.5 and 97.5 quantile predictions for each group
ethnic_predicts <- census_data_predict %>%
  group_by(race_ethnicity_code) %>%
  summarise(mean = mean(our_predict), 
            lower = quantile(our_predict, 0.025), 
            upper = quantile(our_predict, 0.975))

colnames(ethnic_predicts) <- c("Group", "Mean", "Lower", "Upper")

employment_predicts <- census_data_predict %>%
  group_by(employment_code) %>%
  summarise(mean = mean(our_predict), 
            lower = quantile(our_predict, 0.025), 
            upper = quantile(our_predict, 0.975))

colnames(employment_predicts) <- c("Group", "Mean", "Lower", "Upper")

education_predicts <- census_data_predict %>%
  group_by(education_code) %>%
  summarise(mean = mean(our_predict), 
            lower = quantile(our_predict, 0.025), 
            upper = quantile(our_predict, 0.975))

colnames(education_predicts) <- c("Group", "Mean", "Lower", "Upper")

age_predicts <- census_data_predict %>%
  group_by(age_group) %>%
  summarise(mean = mean(our_predict),
            lower = quantile(our_predict, 0.025),
            upper = quantile(our_predict, 0.975))

colnames(age_predicts) <- c("Group", "Mean", "Lower", "Upper")

all_predicts <- rbind(ethnic_predicts, employment_predicts)
all_predicts <- rbind(all_predicts, education_predicts)
all_predicts <- rbind(all_predicts, age_predicts)

rownames(all_predicts) <- c("White", "African American", "American Indian", "Asian", "Other races","Employed", "Unemployed", "Not in labor force", "Middle School", "High School", "University", "Post-graduate", "Ages 18 - 40", "Ages 41 - 60", "Ages 61 - 80", "Ages 81 - 100")

kbl(all_predicts, caption = "American voter intention estimates by race, employment status, education level, and age brackets", booktabs = T, digits = 3) %>%
  kable_styling() %>%
  pack_rows("Race", 1, 5) %>%
  pack_rows("Employment Status", 6, 8) %>%
  pack_rows("Education Status", 9, 12) %>%
  pack_rows("Age Brackets", 13, 16)


```


# Results

Table 1 lists the voter intent estimates by race, employment status, and education level. Overall, Whites and American Indians are estimated to have the highest proportions of voters in favour of Trump ($\hat{y}$ = 0.551 and $\hat{y}$ = 0.557, respectively). Meanwhile, African Americans are estimated to have significantly lower voter intention for Trump ($\hat{y}$ = 0.116). Respondents across all other race categories, employment statuses and education levels have similar intent estimates in favour of the Republican Party ($\hat{y}$ ranges from 0.328 to 0.418).

Of 15840681 observations partitioned into 60 cells in our post-stratified census data, we estimate the proportion of voters in favour of voting for the Republican Party ($\hat{Y}^{PS}$) to be 0.377. This is based on our post-stratification analysis of the proportion of voters in favour of the Republican Party modelled by a logistic regression model, which accounted for three demographic variables: race, employment status, and education level.

```{r, fig.cap = "Histogram of Prediction values"}
census_data_predict %>%
  ggplot(aes(x = our_predict)) +
  theme_minimal() +
  geom_histogram(bins = 30,
                 colour = "black",
                 fill = "transparent") +
  labs(title = "Histogram of Yhat Values",
       x = "Yhat Values",
       y = "Estimated Density")

```

# Discussion

American election forecasts have long been a global interest. In this paper, we predicted the overall popular vote of the 2020 American federal election using a logistic regression model with post-stratification and using datasets from Democracy Fund + UCLA Nationscape and IPUMS, accounting for race, employment status, and education level.

Based on the estimated proportion of voters in favour of voting for the Republican Party being 0.404, we predict that the Democratic Party will win the election. Overall results indicate a deep polarization of voting intention, with most noticeable differences in the area of race. African Americans overwhelmingly intend to support Democratic candidates, which is in line with historical trends (Wallace et al., 2008), while the Whites tend to vote for Republican candidates. 


## Weaknesses

Here we discuss weaknesses of the study, data, analysis, etc. You can also discuss areas for improvement.

There are a number of drawbacks in conducting election forecasts, as well as in our application of the poststratification technique on a logistic model. 

Election forecasts may increase interests in elections and voting, but it could also be radically unreliable, especially for the 2020 presidential election due to polarization and the COVID-19 pandemic, as argued by Jennifer Nicoll Victor, Professor of Political Science at George Mason University (2020). Other downsides include affecting turnout—as evident in the 2016 election when Hiliary Clinton won the popular vote but lost the election, outsizing focus on the Election Horse Race, and giving a false impression of science and uncertainty (Victor, 2020). 

In our model, we only chose four predictor variables to predict the value response variable, but there are more factors that can affect the election result in real life. Also, when we grabbed the census data, we reduced the census data size to 1.25 million, which is smaller than the population size.

Despite MRP being a strong statistical methodology to understand and analyze microdata, in this case, our Census data; it has some glaring weaknesses that impacts our analysis. Some of these errors in MRP include misclassification of our cells, missing data, or missing crucial demographic predictors (Kennedy et al., 2020). For the misclassification of cells, we had to make assumptions when choosing our cell splits, otherwise there would be sample size errors; niche ethnicities like “Samoan”, while technically a Pacific Islander, would be culturally very different than a Malaysian who is also grouped with Pacific Islander. Despite this, we had to maintain consistency between the Survey Data and Census Data so we made the assumption that they are equivocally the same when they are not. Missing data was also an issue during our modelling since for example, employment for the Census Data had the field, “n/a” as a choice. We classified it the same as not in the labour force and assumed that if the respondents were employed/unemployed, they would respond accordingly; obviously this may not be the case since non-sampling error could have been a factor. Lastly, we could also be missing a crucial part of our demographic predictors, with so much microdata, it is important to be careful about the framework we choose for our analysis; some of our cell splits may not have been optimally or correctly classified and thus, lose explanatory power of our claims. Although our model assumptions are intuitively consistent and reasonable, it would be a stretch to say that this is anywhere near perfect.
## Next Steps

Here you discuss subsequent work to be done after this report. This can include next steps in terms of statistical analysis (perhaps there is a more efficient algorithm available, or perhaps there is a caveat in the data that would allow for some new technique). Future steps should also be specified in terms of the study setting (eg. including a follow-up survey on something, or a subsequent study that would complement the conclusions of your report).


# References



